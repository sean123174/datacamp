{"cells":[{"source":"![Clothing Classifier Model](Clothing%20Classifier%20Model.png)\n","metadata":{},"id":"06e96af4-1831-41d9-a3d7-76004d4f01a9","cell_type":"markdown"},{"source":"Fashion Forward is a new AI-based e-commerce clothing retailer.\nThey want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n\nAs a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc.\n","metadata":{"tags":[]},"id":"35d4e17b-eeb6-40dd-a140-7b949390e115","cell_type":"markdown","attachments":{}},{"source":"# Run the cells below first","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1708982732294,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run the cells below first"},"id":"faf2e25e-6c2f-450d-95c1-e03d470cb2f1","cell_type":"code","execution_count":80,"outputs":[]},{"source":"!pip install torchmetrics","metadata":{"executionCancelledAt":null,"executionTime":5466,"lastExecutedAt":1708982737760,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics","outputsMetadata":{"0":{"height":537,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"4de0b0ff-a211-41be-b90e-165e6038c9d7","cell_type":"code","execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.10.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>17.1->torchmetrics) (3.0.9)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n"}]},{"source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall, ConfusionMatrix","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1708982737810,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall, ConfusionMatrix"},"id":"145bbc0f-4d15-4e7b-b796-5ec0d9ab7702","cell_type":"code","execution_count":82,"outputs":[]},{"source":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1708982737873,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())","outputsMetadata":{"0":{"height":77,"type":"stream"},"2":{"height":117,"type":"stream"},"4":{"height":117,"type":"stream"},"6":{"height":117,"type":"stream"},"8":{"height":57,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"35ddad8f-fa43-4bb3-894b-afef8d0bfd59","cell_type":"code","execution_count":83,"outputs":[]},{"source":"# Convert datasets to DataLoader\ndataloader_train = DataLoader(\n  train_data, shuffle=True, batch_size=10\n)\n\ndataloader_test = DataLoader(\n  test_data, shuffle=False, batch_size=1\n)\n","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1708982737926,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert datasets to DataLoader\ndataloader_train = DataLoader(\n  train_data, shuffle=True, batch_size=10\n)\n\ndataloader_test = DataLoader(\n  test_data, shuffle=False, batch_size=1\n)\n"},"cell_type":"code","id":"e122c760-6302-4bd1-8998-4ebc30ae4da8","outputs":[],"execution_count":84},{"source":"# Define Global Variables\n# Get the number of classes\nclasses = train_data.classes\nnum_classes = len(classes)\n\n# Define some relevant variables\nnum_input_channels = 1\nnum_output_channels = 16\nimage_size = train_data[0][0].shape[1]\n\n# Print key inputs\nprint(f\"The number of classes are :{num_classes}\")\nprint(f\"The classes are : {classes}\")\nprint(f\"The Number of Input Channels are {num_input_channels}, the Number of Output Channel are {num_output_channels}, and the Image Size is {image_size}\")\nprint(f\"The length of train is {len(dataloader_train)} and the lenth of test is {len(dataloader_test)}\")\n","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1708982737978,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define Global Variables\n# Get the number of classes\nclasses = train_data.classes\nnum_classes = len(classes)\n\n# Define some relevant variables\nnum_input_channels = 1\nnum_output_channels = 16\nimage_size = train_data[0][0].shape[1]\n\n# Print key inputs\nprint(f\"The number of classes are :{num_classes}\")\nprint(f\"The classes are : {classes}\")\nprint(f\"The Number of Input Channels are {num_input_channels}, the Number of Output Channel are {num_output_channels}, and the Image Size is {image_size}\")\nprint(f\"The length of train is {len(dataloader_train)} and the lenth of test is {len(dataloader_test)}\")\n","outputsMetadata":{"0":{"height":97,"type":"stream"}}},"id":"e5d01322-e517-44b8-a68f-1ff0e291418f","cell_type":"code","execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":"The number of classes are :10\nThe classes are : ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nThe Number of Input Channels are 1, the Number of Output Channel are 16, and the Image Size is 28\nThe length of train is 6000 and the lenth of test is 10000\n"}]},{"source":"# Defining the CNN (Convolutional Neural Network)","metadata":{},"cell_type":"markdown","id":"09d6e171-36d8-422f-a639-37e3ca1c34d1"},{"source":"class Net(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Define feature extractor\n        self.feature_extractor = nn.Sequential(\n        # Convolution layer with kernel size 3 and stride 1\n        nn.Conv2d(in_channels=num_input_channels, out_channels=num_output_channels, kernel_size=3, stride=1, padding=1),\n        # RectiLinear unit activation layer\n        nn.ReLU(),\n        # Max pooling layer with kernel size 2 and stride 2\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        # Flatten\n        nn.Flatten(),\n        )\n        # Define classifier\n        # Fully connected layer\n        self.classifier = nn.Linear(num_output_channels*(image_size//2)**2, num_classes)\n    \n    def forward(self, x):  \n        # Pass input through feature extractor and classifier\n        x = self.feature_extractor(x)\n        x = self.classifier(x)\n        return x","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1708982738030,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"class Net(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # Define feature extractor\n        self.feature_extractor = nn.Sequential(\n        # Convolution layer with kernel size 3 and stride 1\n        nn.Conv2d(in_channels=num_input_channels, out_channels=num_output_channels, kernel_size=3, stride=1, padding=1),\n        # RectiLinear unit activation layer\n        nn.ReLU(),\n        # Max pooling layer with kernel size 2 and stride 2\n        nn.MaxPool2d(kernel_size=2, stride=2),\n        # Flatten\n        nn.Flatten(),\n        )\n        # Define classifier\n        # Fully connected layer\n        self.classifier = nn.Linear(num_output_channels*(image_size//2)**2, num_classes)\n    \n    def forward(self, x):  \n        # Pass input through feature extractor and classifier\n        x = self.feature_extractor(x)\n        x = self.classifier(x)\n        return x"},"cell_type":"code","id":"99b52d18-2ab8-4f1e-881c-80634ed0ed99","outputs":[],"execution_count":86},{"source":"# Train the CNN","metadata":{},"cell_type":"markdown","id":"8f551166-dfcb-4b7b-9703-ed8379b0e54e"},{"source":"# Define the model\nnet = Net(num_classes=num_classes)\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n# Define the optimizer\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nfor epoch in range(1):\n    running_loss = 0.0\n    # Iterate over training batches\n    for images, labels in dataloader_train:\n        optimizer.zero_grad()\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(dataloader_train)\n    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")","metadata":{"executionCancelledAt":null,"executionTime":13606,"lastExecutedAt":1708982751637,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the model\nnet = Net(num_classes=num_classes)\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n# Define the optimizer\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nfor epoch in range(1):\n    running_loss = 0.0\n    # Iterate over training batches\n    for images, labels in dataloader_train:\n        optimizer.zero_grad()\n        outputs = net(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(dataloader_train)\n    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"c7b58724-337f-42b9-bb13-638b8acfed54","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1, Loss: 0.3965\n"}],"execution_count":87},{"source":"# Testing the CNN","metadata":{},"cell_type":"markdown","id":"71ed5911-5e7c-4019-9d59-e7e2311ea31a"},{"source":"# Define metrics\naccuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)\nmetric_confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n\n\n# Run model on test set\nnet.eval()\npredicted = []\nfor i, (features, labels) in enumerate(dataloader_test):\n    output = net.forward(features.reshape(-1, 1, image_size, image_size))\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\n# Compute the metrics\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)\nprint(confusion_matrix)","metadata":{"executionCancelledAt":null,"executionTime":12671,"lastExecutedAt":1708982764308,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define metrics\naccuracy_metric = Accuracy(task='multiclass', num_classes=num_classes)\nprecision_metric = Precision(task='multiclass', num_classes=num_classes, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=num_classes, average=None)\nmetric_confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n\n\n# Run model on test set\nnet.eval()\npredicted = []\nfor i, (features, labels) in enumerate(dataloader_test):\n    output = net.forward(features.reshape(-1, 1, image_size, image_size))\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\n# Compute the metrics\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)\nprint(confusion_matrix)","outputsMetadata":{"0":{"height":317,"type":"stream"}}},"cell_type":"code","id":"dc444868-2529-440a-bf7f-b653be8b6ffc","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.8830999732017517\nPrecision (per class): [0.7856502532958984, 0.9917440414428711, 0.8417112231254578, 0.8743914365768433, 0.80525803565979, 0.9747983813285828, 0.6832432150840759, 0.9419291615486145, 0.9709709882736206, 0.9608040452003479]\nRecall (per class): [0.8759999871253967, 0.9610000252723694, 0.7870000004768372, 0.8980000019073486, 0.8270000219345093, 0.9670000076293945, 0.6320000290870667, 0.9570000171661377, 0.9700000286102295, 0.9559999704360962]\ntensor([[881,   1,  17,  20,   5,   1,  61,   0,  14,   0],\n        [  2, 974,   1,  16,   4,   0,   2,   0,   1,   0],\n        [ 19,   1, 832,   6,  78,   0,  60,   0,   4,   0],\n        [ 30,  16,  15, 879,  27,   0,  30,   0,   3,   0],\n        [  1,   1,  80,  32, 819,   0,  66,   0,   1,   0],\n        [  0,   0,   0,   1,   0, 959,   0,  18,   2,  20],\n        [162,   2,  89,  27,  74,   0, 625,   0,  21,   0],\n        [  0,   0,   0,   0,   0,  13,   0, 891,   1,  95],\n        [  2,   1,   1,   3,   2,   2,   9,   2, 976,   2],\n        [  0,   0,   0,   0,   0,   3,   0,  17,   1, 979]])\n"}],"execution_count":88}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}